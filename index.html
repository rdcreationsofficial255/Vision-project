<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Project VISION AI - Navigator</title>
<style>
body {margin:0; font-family:'Segoe UI',Tahoma,Verdana,sans-serif; background:#111; color:#fff; text-align:center; padding:15px;}
h2 {font-size:28px; margin-bottom:5px; color:#00e676;}
p {font-size:16px; color:#aaa; margin-bottom:20px;}
textarea, input, button {display:block; width:90%; margin:10px auto; border-radius:12px; font-size:18px; padding:12px; border:none; outline:none;}
textarea {height:120px; background:#222; color:#fff; border:2px solid #333;}
input {height:45px; background:#222; color:#fff; border:2px solid #333;}
button {background:linear-gradient(90deg,#00e676,#1de9b6); color:#000; font-weight:bold; box-shadow:0 4px 10px rgba(0,0,0,0.3);}
button:active {transform:scale(0.98); box-shadow:0 2px 6px rgba(0,0,0,0.3);}
video {width:90%; margin-top:15px; border-radius:15px; display:none; border:2px solid #00e676;}
.log {margin-top:20px; font-size:16px; color:#aaa;}
</style>
</head>
<body>

<h2>ðŸ¦¯ Project VISION AI</h2>
<p>Voice â€¢ Text Reader â€¢ Obstacle Detection</p>

<textarea id="textBox" placeholder="Type or paste text here..."></textarea>
<button onclick="readText()">ðŸ”Š Read Text</button>
<button onclick="voiceButtonPress('voice')">ðŸŽ¤ Voice Command</button>
<button onclick="voiceButtonPress('camera')">ðŸ“· Vision / Navigator Mode</button>

<video id="camera" autoplay playsinline></video>
<div class="log" id="log">Waiting for input...</div>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

<script>
// --------------------- GLOBAL VARIABLES --------------------
let cameraOn=false, video=document.getElementById("camera"), model=null, detecting=false, audioContext, lastAlertTime=0;
let previousClass=null, consecutiveCount=0;

// Only real walking obstacles
const obstacleClasses=['person','chair','sofa','table','bed','couch','door','wall','car','bicycle','motorcycle'];

// --------------------- AI VOICE ----------------------------
function speak(text){
  const msg=new SpeechSynthesisUtterance(text);
  msg.lang='en-IN'; msg.rate=1;
  window.speechSynthesis.speak(msg);
  document.getElementById("log").innerText=text;
}

// --------------------- TEXT READER ------------------------
function readText(){
  const txt=document.getElementById("textBox").value.trim();
  if(!txt){ speak("Please type some text first."); return; }
  speak(txt);
}

// --------------------- VOICE COMMAND ----------------------
function voiceButtonPress(action){
  if(action==='voice') startListening();
  else if(action==='camera') toggleCamera();
}

// --------------------- VOICE RECOGNITION -----------------
function startListening(){
  speak("Listening for your command.");
  try{
    const SpeechRecognition=window.SpeechRecognition||window.webkitSpeechRecognition;
    if(!SpeechRecognition){ speak("Voice recognition not supported."); return; }
    const rec=new SpeechRecognition();
    rec.lang='en-IN';
    rec.onresult=function(e){
      const cmd=e.results[0][0].transcript.toLowerCase();
      handleCommand(cmd);
    };
    rec.start();
  }catch(e){ speak("Voice recognition not supported."); }
}

function handleCommand(cmd){
  if(cmd.includes("read")) readText();
  else if(cmd.includes("clear")){ document.getElementById("textBox").value=""; speak("Text cleared."); }
  else if(cmd.includes("camera")||cmd.includes("vision")||cmd.includes("navigator")) toggleCamera();
  else speak("Command not recognized.");
}

// --------------------- CAMERA / OBJECT DETECTION ----------
async function toggleCamera(){
  if(!cameraOn){
    try{
      video.style.display="block";
      const stream=await navigator.mediaDevices.getUserMedia({video:{facingMode:"environment"}});
      video.srcObject=stream;
      speak("Loading vision model...");
      model=await cocoSsd.load();
      speak("Vision ready. I will alert for real obstacles.");
      cameraOn=true; detecting=true; detectObjects();
    }catch(e){ speak("Cannot access camera."); console.error(e);}
  }else stopCamera();
}

function stopCamera(){ 
  if(video.srcObject) video.srcObject.getTracks().forEach(t=>t.stop()); 
  video.style.display="none"; cameraOn=false; detecting=false; 
  speak("Camera turned off."); 
}

async function detectObjects(){
  if(!model||!detecting) return;
  const preds=await model.detect(video);
  if(preds.length>0){
    const top=preds[0];
    // Filter only relevant obstacles & large objects
    const area=top.bbox[2]*top.bbox[3];
    if(obstacleClasses.includes(top.class) && area>5000){
      if(top.class===previousClass) consecutiveCount++;
      else consecutiveCount=1;
      previousClass=top.class;

      if(consecutiveCount>=2){
        const distance=getDistanceFromArea(area);
        if(Date.now()-lastAlertTime>2500){
          if(distance<0.5) speak("Stop! Obstacle very close, turn left slowly.");
          else if(distance<1) speak("Obstacle ahead, move slowly.");
          else speak("Obstacle detected ahead, continue carefully.");
          playBeep(distance<0.5?300:700);
          lastAlertTime=Date.now();
        }
      }
    }
  }
  requestAnimationFrame(detectObjects);
}

function getDistanceFromArea(area){
  if(area>120000) return 0.3;
  if(area>60000) return 0.6;
  if(area>20000) return 1.0;
  return 2.0;
}

function playBeep(freq){ 
  if(!audioContext) audioContext=new AudioContext(); 
  const osc=audioContext.createOscillator(); 
  const gain=audioContext.createGain(); 
  osc.connect(gain); gain.connect(audioContext.destination); 
  osc.type="sine"; osc.frequency.value=freq; osc.start(); 
  gain.gain.exponentialRampToValueAtTime(0.0001,audioContext.currentTime+0.3); 
}
</script>
</body>
</html>